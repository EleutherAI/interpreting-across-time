{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02c0ece4",
   "metadata": {},
   "source": [
    "# Install the prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef80fc3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt install zstd -y\n",
    "!pip install transformers datasets 'torch>=1.9.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327967bc",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0415a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_datasets, load_dataset\n",
    "datasets_list = list_datasets()\n",
    "\n",
    "src_path = 'glue'\n",
    "src_name = 'mrpc'\n",
    "src = src_path + \"-\" + src_name\n",
    "\n",
    "dataset_glue = load_dataset(src_path, src_name, split='train')\n",
    "\n",
    "dataset = []\n",
    "idx = 0\n",
    "for record in dataset_glue:\n",
    "    if 'sentence1' in record:\n",
    "        dataset.append({'text': record['sentence1'], 'source': src, 'index': \"{}.s1\".format(idx)})\n",
    "    if 'sentence2' in record:\n",
    "        dataset.append({'text': record['sentence2'], 'source': src, 'index': \"{}.s2\".format(idx)})\n",
    "\n",
    "dataset_name = src\n",
    "print('Dataset has', len(dataset), 'examples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00112945",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bf2a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, GPTJForCausalLM\n",
    "\n",
    "layers = list(range(28))\n",
    "device_ids = list(range(2))\n",
    "assert len(device_ids) % len(device_ids) == 0\n",
    "num_layers_per_device = len(layers) // len(device_ids)\n",
    "device_map = { i: layers[i * num_layers_per_device : (i + 1) * num_layers_per_device] for i in device_ids }\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", low_cpu_mem_usage=True)\n",
    "model.parallelize(device_map)\n",
    "\n",
    "print(\"Model Loaded..!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e504982a",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79bf506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register hooks\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "# make sure we clear up any existing hooks if this cell is re-run\n",
    "if 'handles' in globals():\n",
    "    print('Removing existing hooks...')\n",
    "    for handle in handles:\n",
    "        handle.remove()\n",
    "\n",
    "module_names_to_track_for_activations = \\\n",
    "    [f'transformer.h.{i}.mlp.fc_out' for i in range(model.config.n_layer)]\n",
    "module_names_to_track_for_logits = \\\n",
    "    [f'transformer.h.{i}' for i in range(model.config.n_layer)]\n",
    "\n",
    "hidden_states = dict()\n",
    "def save_hidden_states(name, module, input, output):\n",
    "    # we care about input for activations, and output for logit lens\n",
    "    hidden_states[name] = { 'input': input[0], 'output': output }\n",
    "\n",
    "def clear_hidden_states():\n",
    "    hidden_states = dict()\n",
    "\n",
    "handles = []\n",
    "cnt = 0\n",
    "for name, m in model.named_modules():\n",
    "    if name in module_names_to_track_for_activations or name in module_names_to_track_for_logits:\n",
    "        cnt += 1\n",
    "        handle = m.register_forward_hook(partial(save_hidden_states, name))\n",
    "        handles.append(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d08145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some settings\n",
    "\n",
    "num_tokens = 10000\n",
    "top_k = 5\n",
    "activation_threshold = 3\n",
    "dense_activations = False\n",
    "\n",
    "# stop after this many records in the dataset have been processed\n",
    "dataset_limit = 10\n",
    "\n",
    "# how many bytes to pickle into a single file before compressing it and starting a new file\n",
    "file_size_goal = 128 * 1024 * 1024\n",
    "\n",
    "main_device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053fd118",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf output\n",
    "!mkdir output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993bbb22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class batch_pickler:\n",
    "    file_name_base = None\n",
    "    file_bytes_goal = 0\n",
    "    save_file_func = None\n",
    "\n",
    "    next_file_idx = 0\n",
    "\n",
    "    file = None\n",
    "    cur_file_name = None\n",
    "\n",
    "    def __init__(self, file_name_base, file_bytes_goal, save_file_func):\n",
    "        self.file_name_base = file_name_base\n",
    "        self.file_bytes_goal = file_bytes_goal\n",
    "        self.save_file_func = save_file_func\n",
    "\n",
    "    def dump(self, record):\n",
    "        # if the current batch is full, close it\n",
    "        if self.file is not None and self.file.tell() > self.file_bytes_goal:\n",
    "            self.close()\n",
    "\n",
    "        # if there is no open batch, create a new one\n",
    "        if self.file is None:\n",
    "            self.cur_file_name = \"{}.{}\".format(self.file_name_base, self.next_file_idx)\n",
    "            self.file = open(self.cur_file_name, 'wb')\n",
    "            self.next_file_idx = self.next_file_idx + 1\n",
    "\n",
    "        # pickle the record into the current batch\n",
    "        pickle.dump(record, self.file)\n",
    "\n",
    "    def close(self):\n",
    "        if self.file != None:\n",
    "            # close the file\n",
    "            self.file.close()\n",
    "            self.file = None\n",
    "\n",
    "            # compress and upload the file\n",
    "            self.save_file_func(self.cur_file_name)\n",
    "\n",
    "            # remove the file\n",
    "            os.remove(self.cur_file_name)\n",
    "\n",
    "            # mark the batch as finished\n",
    "            self.cur_file_name = None\n",
    "\n",
    "def extract_neuron_values(key, threshold, dense=True):\n",
    "    \"\"\"\n",
    "    For each MLP, determine which neurons fire at any point during the entire sequence,\n",
    "    unless it only fires on the first token (which we will just assume is noise).\n",
    "\n",
    "    The output is a list of dicts resembling individual neurons with fields:\n",
    "        l: the layer of the neuron\n",
    "        f: the index of the neuron in the feature dimension\n",
    "        a: a list of activations equal to the length of the sequence\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    neurons = []\n",
    "    for name in module_names_to_track_for_activations:\n",
    "        h = hidden_states[name][key]\n",
    "        neurons.append(h[0].to(main_device))\n",
    "    neurons = torch.stack(neurons)\n",
    "\n",
    "    if dense:\n",
    "        return neurons.tolist()\n",
    "\n",
    "    high_activations = (neurons > threshold).nonzero()\n",
    "    values = []\n",
    "    uniq = set()\n",
    "    i = 0\n",
    "    st = time.time()\n",
    "    torch.index_select(neurons, 0, high_activations[:, 0])\n",
    "    return None\n",
    "    for layer_idx, _, feature_idx in high_activations:\n",
    "        layer_idx = layer_idx.item()\n",
    "        feature_idx = feature_idx.item()\n",
    "        if (layer_idx, feature_idx) in uniq:\n",
    "            # we already have it!\n",
    "            continue\n",
    "        uniq.add((layer_idx, feature_idx))\n",
    "        values.append({\n",
    "            'l': layer_idx,\n",
    "            'f': feature_idx,\n",
    "            'a': neurons[layer_idx, :, feature_idx].reshape([neurons.shape[1]]).tolist(),\n",
    "        })\n",
    "        i += 1\n",
    "        if i % 10000 == 0:\n",
    "            pass\n",
    "#             print(i, len(uniq))\n",
    "    en = time.time()\n",
    "    print('here:', en-st)\n",
    "    return values\n",
    "\n",
    "def extract_logit_lens(k=10):\n",
    "    \"\"\"\n",
    "    Extract the output logits for each layer (including the final layer)\n",
    "\n",
    "    Returns a nested list structure of shape [n_layers, n_seq, k]\n",
    "    where each element is a dict containing:\n",
    "        tok: the predicted token\n",
    "        prob: the probability given to this token (from softmax of logits)\n",
    "\n",
    "    Note: The sum of the final dimension probabilities will be very close to 1.\n",
    "    \"\"\"\n",
    "\n",
    "    per_layer_tokens = []\n",
    "    for name in module_names_to_track_for_logits:\n",
    "        h2 = hidden_states[name]['output'][0]  # x, present\n",
    "        with torch.no_grad():\n",
    "            layer_logits = model.lm_head(h2.to(main_device)).detach()[0]\n",
    "        seq = layer_logits.shape[0]\n",
    "        values, indices = torch.topk(layer_logits, k=k)\n",
    "        norm_values = F.softmax(values, dim=-1)\n",
    "        indices = indices.cpu()\n",
    "        norm_values = norm_values.cpu()\n",
    "        top_in_sequence = []\n",
    "        for i in range(seq):\n",
    "            top_tokens = []\n",
    "            for tok, prob in zip(indices[i], norm_values[i]):\n",
    "                tok = tok.item()\n",
    "                prob = prob.item()\n",
    "                top_tokens.append({\n",
    "                    'tok': tokenizer.decode([tok]),\n",
    "                    'prob': prob,\n",
    "                })\n",
    "            top_in_sequence.append(top_tokens)\n",
    "        per_layer_tokens.append(top_in_sequence)\n",
    "    return per_layer_tokens\n",
    "\n",
    "def compress_upload(file_name):\n",
    "    file_name_zst = file_name + \".zst\"\n",
    "\n",
    "    # compress the file with zstd\n",
    "    runres = subprocess.run([\"zstd\", file_name, \"-f\", \"-o\", file_name_zst])\n",
    "    if runres.returncode != 0:\n",
    "        raise Exception(\"zstd compression failed. ec={}\".format(\n",
    "            runres.returncode))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "total_records = 0\n",
    "total_tokens = 0\n",
    "\n",
    "try:\n",
    "    pickler = batch_pickler(\"output/neurons.pickle\", file_size_goal, compress_upload)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for row in tqdm(dataset):\n",
    "            inputs = tokenizer(row['text'], return_tensors=\"pt\")\n",
    "            context = inputs[\"input_ids\"][0][:num_tokens]\n",
    "            context = context.to(main_device)\n",
    "\n",
    "            total_records = total_records + 1\n",
    "            total_tokens = total_tokens + len(context)\n",
    "\n",
    "            clear_hidden_states()\n",
    "\n",
    "            output = model(context, return_dict=True, output_attentions=True)\n",
    "\n",
    "            st = time.time()\n",
    "            activations_in = extract_neuron_values('input', activation_threshold, dense_activations)\n",
    "            en = time.time()\n",
    "            print('process in:', en-st)\n",
    "            st = time.time()\n",
    "            activations_out = extract_neuron_values('output', activation_threshold, dense_activations)\n",
    "            en = time.time()\n",
    "            print('process out:', en-st)\n",
    "            st = time.time()\n",
    "            logits = extract_logit_lens(k=top_k)  # [48, seq, 5]\n",
    "            en = time.time()\n",
    "            print('logit lens:', en-st)\n",
    "            st = time.time()\n",
    "            attentions = torch.stack([a.to(main_device) for a in output.attentions]).tolist()\n",
    "            en = time.time()\n",
    "            print('attn:', en-st)\n",
    "\n",
    "            record = {\n",
    "                'text': row[\"text\"],\n",
    "                'source': row['source'],\n",
    "                'tokens': context,\n",
    "                'activationsIn': activations_in,\n",
    "                'activationsOut': activations_out,\n",
    "                'logits': logits,\n",
    "                'attentions': attentions,\n",
    "            }\n",
    "\n",
    "            pickler.dump(record)\n",
    "            if total_records == dataset_limit:\n",
    "                break\n",
    "finally:\n",
    "    pickler.close()\n",
    "\n",
    "print(\"Records: \", total_records)\n",
    "print(\"Processed tokens: \", total_tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
